{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVmT47LOBy6i"
      },
      "source": [
        " <center> <h1> <b> Pattern Recognition and Machine Learning (EE5610 - EE2802 - AI2000 - AI5000) </b> </h1> </center>\n",
        "\n",
        "<b> Programming Assignment - 04 : Neural Networks </b>\n",
        "\n",
        "\n",
        "This programming assignment gives you a chance to perform the classification task using neural networks. You will get to build a neural network from scratch and train and test it on a standard classification dataset. Further you will learn different tricks and techniques to train a neural network eficiently by observing few important issues and trying to overcome them. This includes observing the performance of the network for different activation functions and optimization algorithms. We will conclude with implementation of various regularization techniques to overcome the problems of overfitting and vanishing gradients.\n",
        "\n",
        "<b> Instructions </b>\n",
        "1. Plagiarism is strictly prohibited.\n",
        "2. Delayed submissions will be penalized with a scaling factor of 0.5 per day.\n",
        "3. Please DO NOT use any machine learning libraries unless and otherwise specified.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYAp-ibURvdd"
      },
      "source": [
        "<b>  Part - (1) : Develop a neural network based classification network from scratch: </b>  This programming assignment uses MNIST digit dataset. It consists of large collection of handwritten digits from 0 to 9. These images are formated as 28x28 pixel gray scale images. The objective of this programming assignment is to design a neural network architecture that takes input as 28x28 image (or 784 dimensional vector) as input and predicts the digit information in it. Although there are diffrent varieties of neural network architecture to solve this task, this programming assignment uses only the feed forward network.  \n",
        "\n",
        "<dt> <h6> 1. Load MNIST data and create train, test splits </dt> </h6>\n",
        "\n",
        "<dd> <h6> - The MNIST dataset consists of around 70,000 images. Divide the dataset into two segments: training and testing. Allocate 60,000 images for training and 10,000 images for testing\n",
        "</dd> </h6>\n",
        "<dd> <h6> - Code for downloading the data and creating train-test splits is provided </dd> </h6>\n",
        "\n",
        "<dt> <h6> 2. Design a simple classification network </dt> </h6>\n",
        "\n",
        "<dd> <h6> - Let us use three layer feed-forward neral network. Use 512 nodes in the hidden layers and 10 nodes in the output layer. The output $\\textbf{y}$ from the input $\\textbf{x}$ is computed as follows </dd> </h6>\n",
        "<dd> <h6> <center> $ \\textbf{y} = h(\\textbf{W}_{3}g(\\textbf{W}_{2}g(\\textbf{W}_{1}\\textbf{x}))) $ </center> </dd> </h6>\n",
        "<dd> <h6> where $\\textbf{W}_{1} \\in \\mathcal{R}^{512 \\times 768}$,$\\textbf{W}_{2} \\in \\mathcal{R}^{512 \\times 512}$,$\\textbf{W}_{3} \\in \\mathcal{R}^{10 \\times 512} $ are the parameters of the network. g(.) is the hidden layer activation function. h(.) is the output layer activation function   </dd> </h6>\n",
        "<dd> <h6> - Consider g(.) as ReLU activation function. Softmax activation function should be used at the last layer h(.), to get the posterior probability of the classes. </dd> </h6>\n",
        "\n",
        "<dt> <h6> - Training classification network: </dt> </h6>\n",
        "\n",
        "<dd> <h6>  - Flatten the 28x28 images to arrive at 784 dimensional vector.  </dd> </h6>  \n",
        "<dd> <h6> - Randomly initialize the parameters of network, $\\textbf{W}_{1} \\in \\mathcal{R}^{768 \\times 512}$,$\\textbf{W}_{2} \\in \\mathcal{R}^{512 \\times 512}$,$\\textbf{W}_{3} \\in \\mathcal{R}^{512 \\times 10}$  </dd> </h6>\n",
        "<dd> <h6> - Feedforward the batch of input vectors to get the posterior probability of classes.  </dd> </h6>\n",
        "<dd> <h6> - Compute the loss between the estimated posterior probabilities and the true targets. </dd> </h6>  \n",
        "<dd> <h6> - Update the parameters of network to minimize the loss function.  </dd> </h6>\n",
        "<dd> <h6> <dd> <h6> - Backpropagate the loss function to get the gradients.  </dd> </h6> </dd> </h6>  \n",
        "\n",
        "<dd> <h6> <dd> <h6> - You can use stochastic gradient descent (SGD) optimization algorithm to update the parameters.  </dd> </h6> </dd> </h6>  \n",
        "<dd> <h6> <dd> <h6> - Cleverly set the hyperparameters involved in this optimization process. </dd> </h6> </dd> </h6>\n",
        "\n",
        "<dt> <h6> 3. Evaluate the performance of classification network </dd> </h6>\n",
        "<dd> <h6> - feed-forward the MNIST data through the trained classification network to get class posteriors. </dd> </h6>  \n",
        "<dd> <h6> - Assign the input to the class having maximum posterior probability </dd> </h6>  \n",
        "<dd> <h6> - Compute the loss and accuaracy </dd> </h6>  \n",
        "<dd> <h6> - Report your observations </dd> </h6>  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "63TGxyPNSSEX"
      },
      "outputs": [],
      "source": [
        "#All imports\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data (60000, 28, 28)\n",
            "Training labels (60000,)\n",
            "Testing data (10000, 28, 28)\n",
            "Testing labels (10000,)\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "#Load MNIST data.\n",
        "##################################################\n",
        "import torchvision.datasets as datasets\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
        "\n",
        "#Training data\n",
        "mnist_traindata = mnist_trainset.data.numpy()\n",
        "mnist_trainlabel = mnist_trainset.targets.numpy()\n",
        "print(\"Training data\",mnist_traindata.shape)\n",
        "print(\"Training labels\",mnist_trainlabel.shape)\n",
        "\n",
        "#Testing data\n",
        "mnist_testdata = mnist_testset.data.numpy()\n",
        "mnist_testlabel = mnist_testset.targets.numpy()\n",
        "print(\"Testing data\",mnist_testdata.shape)\n",
        "print(\"Testing labels\",mnist_testlabel.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eEbdtJuATJKS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30 - Training Accuracy: 0.9226296296296296, Validation Accuracy: 0.9368333333333333\n",
            "Epoch 2/30 - Training Accuracy: 0.9432962962962963, Validation Accuracy: 0.9541666666666667\n",
            "Epoch 3/30 - Training Accuracy: 0.9563518518518519, Validation Accuracy: 0.9645\n",
            "Epoch 4/30 - Training Accuracy: 0.9605555555555556, Validation Accuracy: 0.9671666666666666\n",
            "Epoch 5/30 - Training Accuracy: 0.9667592592592592, Validation Accuracy: 0.967\n",
            "Epoch 6/30 - Training Accuracy: 0.9730740740740741, Validation Accuracy: 0.9723333333333334\n",
            "Epoch 7/30 - Training Accuracy: 0.9773518518518518, Validation Accuracy: 0.974\n",
            "Epoch 8/30 - Training Accuracy: 0.9798703703703704, Validation Accuracy: 0.9753333333333334\n",
            "Epoch 9/30 - Training Accuracy: 0.9814259259259259, Validation Accuracy: 0.9746666666666667\n",
            "Epoch 10/30 - Training Accuracy: 0.982925925925926, Validation Accuracy: 0.9766666666666667\n",
            "Epoch 11/30 - Training Accuracy: 0.9852222222222222, Validation Accuracy: 0.9761666666666666\n",
            "Epoch 12/30 - Training Accuracy: 0.9872592592592593, Validation Accuracy: 0.9773333333333334\n",
            "Epoch 13/30 - Training Accuracy: 0.9865185185185186, Validation Accuracy: 0.9776666666666667\n",
            "Epoch 14/30 - Training Accuracy: 0.9896666666666667, Validation Accuracy: 0.9788333333333333\n",
            "Epoch 15/30 - Training Accuracy: 0.9907222222222222, Validation Accuracy: 0.9788333333333333\n",
            "Epoch 16/30 - Training Accuracy: 0.9919074074074075, Validation Accuracy: 0.9785\n",
            "Epoch 17/30 - Training Accuracy: 0.991462962962963, Validation Accuracy: 0.9783333333333334\n",
            "Epoch 18/30 - Training Accuracy: 0.9936481481481482, Validation Accuracy: 0.98\n",
            "Epoch 19/30 - Training Accuracy: 0.9944814814814815, Validation Accuracy: 0.9813333333333333\n",
            "Epoch 20/30 - Training Accuracy: 0.9947407407407407, Validation Accuracy: 0.9798333333333333\n",
            "Epoch 21/30 - Training Accuracy: 0.9952962962962963, Validation Accuracy: 0.9796666666666667\n",
            "Epoch 22/30 - Training Accuracy: 0.9966296296296296, Validation Accuracy: 0.9793333333333333\n",
            "Epoch 23/30 - Training Accuracy: 0.9968148148148148, Validation Accuracy: 0.9805\n",
            "Epoch 24/30 - Training Accuracy: 0.9968333333333333, Validation Accuracy: 0.981\n",
            "Epoch 25/30 - Training Accuracy: 0.9975740740740741, Validation Accuracy: 0.9808333333333333\n",
            "Epoch 26/30 - Training Accuracy: 0.9976851851851852, Validation Accuracy: 0.981\n",
            "Epoch 27/30 - Training Accuracy: 0.9981296296296296, Validation Accuracy: 0.983\n",
            "Epoch 28/30 - Training Accuracy: 0.9980740740740741, Validation Accuracy: 0.9815\n",
            "Epoch 29/30 - Training Accuracy: 0.9987592592592592, Validation Accuracy: 0.9818333333333333\n",
            "Epoch 30/30 - Training Accuracy: 0.9989074074074074, Validation Accuracy: 0.9816666666666667\n",
            "Test Accuracy: 0.9801\n"
          ]
        }
      ],
      "source": [
        "\n",
        "##################################################\n",
        "#Define the architecture\n",
        "##################################################\n",
        "\n",
        "#Complete the below function to impliment ReLU activation function\n",
        "def ReLu(inp):\n",
        "\n",
        "  return np.maximum(np.zeros(inp.shape), inp)\n",
        "\n",
        "#Complete the below function to impliment gradient of ReLU activation function\n",
        "def gradReLu(inp):\n",
        "\n",
        "  return np.where(inp > 0, 1, 0)\n",
        "\n",
        "#Complete the below function to impliment softmax activation function\n",
        "def softmax(inp):\n",
        "  exps = np.exp(inp)\n",
        "  sum_exps = np.sum(exps)\n",
        "  outp =  exps / sum_exps\n",
        "  return outp\n",
        "\n",
        "#Complete the below function to impliment forward propagation of data\n",
        "\n",
        "# Complete the below function to implement forward propagation of data\n",
        "def fwdPropagate(inputs, weights):\n",
        "    # Inputs: input data, parameters of network\n",
        "    W1, W2, W3 = weights\n",
        "\n",
        "    # Forward pass\n",
        "    z1 = np.dot(inputs,W1.T)\n",
        "    a1 = ReLu(z1)\n",
        "    z2 = np.dot(a1, W2.T)\n",
        "    a2 = ReLu(z2)\n",
        "    z3 = np.dot(a2,W3.T)\n",
        "    y = np.array([softmax(i) for i in z3])\n",
        "\n",
        "    # Return the requires outputs, i.e., final output and intermediate activations\n",
        "    return [a1, a2, y, z1, z2]\n",
        "\n",
        "\n",
        "#Complete the below function to compute the gradients\n",
        "def computeGradients(inputs, targets, weights, activations):\n",
        "    # Inputs: input data, targets, parameters of network, intermediate activations\n",
        "    W1, W2, W3 = weights\n",
        "    a1, a2, y, z1, z2 = activations\n",
        "\n",
        "    # One-hot encode the targets\n",
        "    targets_onehot = np.eye(10)[targets]\n",
        "\n",
        "    # Compute loss\n",
        "    # loss = -np.sum(targets_onehot @ np.log(out puts.T)) / len(targets)\n",
        "\n",
        "    # Compute gradients\n",
        "    delta3 = (y - targets_onehot)\n",
        "    delta2 = np.dot(delta3, W3) * gradReLu(z2)\n",
        "    delta1 = np.dot(delta2, W2) * gradReLu(z1)\n",
        "\n",
        "    dj_dw3 = np.dot(delta3.T, a2)\n",
        "    dj_dw2 = np.dot(delta2.T, a1)\n",
        "    dj_dw1 = np.dot(delta1.T, inputs)\n",
        "\n",
        "    # Return the gradients and loss\n",
        "    return [dj_dw1, dj_dw2, dj_dw3]\n",
        "\n",
        "\n",
        "#Complete the below function to update the parameters using the above computed gradients\n",
        "def applyGradients(weights, gradients, learning_rate):\n",
        "  # Inputs: weights, gradients, and learning rate\n",
        "  W1, W2, W3 = weights\n",
        "  dj_dw1, dj_dw2, dj_dw3 = gradients\n",
        "  # Update the parameters\n",
        "  W1 -= learning_rate * dj_dw1\n",
        "  W2 -= learning_rate * dj_dw2\n",
        "  W3 -= learning_rate * dj_dw3\n",
        "  # Return the updated parameters\n",
        "  return [W1, W2, W3]\n",
        "\n",
        "#Complete the below function to complete the backpropagation ste\n",
        "def backPropagate(inputs, targets, weights, activations, learning_rate):\n",
        "    # Compute gradients\n",
        "    gradients = computeGradients(inputs, targets, weights, activations)\n",
        "\n",
        "    # Update weights\n",
        "    weights = applyGradients(weights, gradients, learning_rate)\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Train the network\n",
        "##################################################\n",
        "\n",
        "#Complete the below function to complete the training of network\n",
        "# Train the network\n",
        "def training(inputs, targets_idx, batch_size=300, epochs=30, train_val_split=0.9, learning_rate=0.00001):\n",
        "    # Set the hyperparameters\n",
        "    hidden_units = 512\n",
        "    n_classes = 10\n",
        "    n_samples = inputs.shape[0]\n",
        "\n",
        "    # Split the training data into training and validation sets\n",
        "    split_idx = int(n_samples * train_val_split)\n",
        "    inputs_train, inputs_val = inputs[:split_idx], inputs[split_idx:]\n",
        "    targets_train, targets_val = targets_idx[:split_idx], targets_idx[split_idx:]\n",
        "\n",
        "    n_batches = split_idx // batch_size\n",
        "\n",
        "    # Randomly initialize the weights\n",
        "    W1 = np.random.randn(hidden_units, inputs.shape[1]) * 0.01\n",
        "    W2 = np.random.randn(hidden_units, hidden_units) * 0.01\n",
        "    W3 = np.random.randn(n_classes, hidden_units) * 0.01\n",
        "    weights = [W1, W2, W3]\n",
        "\n",
        "    # Iterate for epochs times\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle the training data\n",
        "        indices = np.random.permutation(split_idx)\n",
        "        inputs_train_shuffled = inputs_train[indices]\n",
        "        targets_train_shuffled = targets_train[indices]\n",
        "\n",
        "        # Iterate through the batches of data\n",
        "        for batch in range(n_batches):\n",
        "            # Get the batch of data\n",
        "            start = batch * batch_size\n",
        "            end = min((batch + 1) * batch_size, split_idx)\n",
        "            inputs_batch = inputs_train_shuffled[start:end]\n",
        "            targets_batch = targets_train_shuffled[start:end]\n",
        "\n",
        "            # Forward propagation\n",
        "            activations = fwdPropagate(inputs_batch, weights)\n",
        "\n",
        "            # Backward propagation\n",
        "            weights = backPropagate(inputs_batch, targets_batch, weights, activations, learning_rate)\n",
        "\n",
        "        # Compute outputs on training data\n",
        "        _, _, outputs_train, _, _ = fwdPropagate(inputs_train, weights)\n",
        "        predicted_labels_train = np.argmax(outputs_train, axis=1)\n",
        "        training_accuracy = np.mean(predicted_labels_train == targets_train)\n",
        "\n",
        "        # Compute outputs on validation data\n",
        "        _, _, outputs_val, _, _ = fwdPropagate(inputs_val, weights)\n",
        "        predicted_labels_val = np.argmax(outputs_val, axis=1)\n",
        "        validation_accuracy = np.mean(predicted_labels_val == targets_val)\n",
        "\n",
        "        # Print statistics\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Training Accuracy: {training_accuracy}, Validation Accuracy: {validation_accuracy}\")\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "# Call the training function to train the network\n",
        "trained_weights = training(mnist_traindata.reshape(-1, 784), mnist_trainlabel)\n",
        "\n",
        "##################################################\n",
        "#Evaluate the performance on test data\n",
        "##################################################\n",
        "# Complete the below function to evaluate performance on test data\n",
        "def evaluate(inputs, targets_idx, weights):\n",
        "    # Forward pass\n",
        "    _, _, outputs, _, _ = fwdPropagate(inputs, weights)\n",
        "    \n",
        "    # Assign inputs to the class having maximum posterior probability\n",
        "    preds = np.argmax(outputs, axis=1)\n",
        "    \n",
        "    # Compute accuracy\n",
        "    accuracy = np.mean(preds == targets_idx)\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Evaluate performance on test data\n",
        "test_accuracy = evaluate(mnist_testdata.reshape(-1, 784), mnist_testlabel, trained_weights)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFfifc-c5wqM"
      },
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1. The training accuracy consistently increases as the number of epochs increases. This indicates that the neural network is learning the features and becoming more accurate in classifying the training data over time\n",
        "\n",
        "2. The test accuracy, which measures the performance of the model on unseen test data, is approximately 98.01%. This indicates that the model is performing well and generalizing effectively to new, unseen examples\n",
        "\n",
        "3.  The training loss decreases with each epoch, indicating that the model is converging towards a minimum of the loss function. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AUF8zcrO1kd"
      },
      "source": [
        "<b> Part - (2) : Understanding activation functions: </b> In this part you will learn to use different activation functions for the classification task and compare their performances.\n",
        "\n",
        "<dt> <h6> 1. Train MNIST digit classification network with different activation functions i.e. Sigmoid, Tanh, ReLU, LeakyReLU etc. You can stick to stochastic gradient descent optimization algorithm for this part </dt> </h6>\n",
        "<dt> <h6> 2. Report the accuray on MNIST test data for all the experiments. Write down your observations in the report.</br> </dt> </h6>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4HNrJP_1UyDn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training with sigmoid activation function:\n",
            "Epoch 1/30 - Training Accuracy: 0.11318518518518518, Validation Accuracy: 0.105\n",
            "Epoch 2/30 - Training Accuracy: 0.11318518518518518, Validation Accuracy: 0.105\n",
            "Epoch 3/30 - Training Accuracy: 0.11403703703703703, Validation Accuracy: 0.10616666666666667\n",
            "Epoch 4/30 - Training Accuracy: 0.11318518518518518, Validation Accuracy: 0.105\n",
            "Epoch 5/30 - Training Accuracy: 0.2605925925925926, Validation Accuracy: 0.263\n",
            "Epoch 6/30 - Training Accuracy: 0.2664074074074074, Validation Accuracy: 0.27316666666666667\n",
            "Epoch 7/30 - Training Accuracy: 0.3466296296296296, Validation Accuracy: 0.3515\n",
            "Epoch 8/30 - Training Accuracy: 0.41894444444444445, Validation Accuracy: 0.431\n",
            "Epoch 9/30 - Training Accuracy: 0.37862962962962965, Validation Accuracy: 0.3915\n",
            "Epoch 10/30 - Training Accuracy: 0.47103703703703703, Validation Accuracy: 0.4856666666666667\n",
            "Epoch 11/30 - Training Accuracy: 0.477, Validation Accuracy: 0.49783333333333335\n",
            "Epoch 12/30 - Training Accuracy: 0.49512962962962964, Validation Accuracy: 0.5156666666666667\n",
            "Epoch 13/30 - Training Accuracy: 0.5252962962962963, Validation Accuracy: 0.5473333333333333\n",
            "Epoch 14/30 - Training Accuracy: 0.5818518518518518, Validation Accuracy: 0.6091666666666666\n",
            "Epoch 15/30 - Training Accuracy: 0.5954259259259259, Validation Accuracy: 0.6211666666666666\n",
            "Epoch 16/30 - Training Accuracy: 0.5957407407407408, Validation Accuracy: 0.6228333333333333\n",
            "Epoch 17/30 - Training Accuracy: 0.6155, Validation Accuracy: 0.6443333333333333\n",
            "Epoch 18/30 - Training Accuracy: 0.6175, Validation Accuracy: 0.6425\n",
            "Epoch 19/30 - Training Accuracy: 0.6500185185185186, Validation Accuracy: 0.6771666666666667\n",
            "Epoch 20/30 - Training Accuracy: 0.6560370370370371, Validation Accuracy: 0.6771666666666667\n",
            "Epoch 21/30 - Training Accuracy: 0.6819814814814815, Validation Accuracy: 0.7118333333333333\n",
            "Epoch 22/30 - Training Accuracy: 0.710037037037037, Validation Accuracy: 0.743\n",
            "Epoch 23/30 - Training Accuracy: 0.7163888888888889, Validation Accuracy: 0.7485\n",
            "Epoch 24/30 - Training Accuracy: 0.7322962962962963, Validation Accuracy: 0.7598333333333334\n",
            "Epoch 25/30 - Training Accuracy: 0.7598888888888888, Validation Accuracy: 0.7891666666666667\n",
            "Epoch 26/30 - Training Accuracy: 0.7824629629629629, Validation Accuracy: 0.8123333333333334\n",
            "Epoch 27/30 - Training Accuracy: 0.7961111111111111, Validation Accuracy: 0.8266666666666667\n",
            "Epoch 28/30 - Training Accuracy: 0.8117777777777778, Validation Accuracy: 0.8393333333333334\n",
            "Epoch 29/30 - Training Accuracy: 0.826462962962963, Validation Accuracy: 0.8531666666666666\n",
            "Epoch 30/30 - Training Accuracy: 0.8334444444444444, Validation Accuracy: 0.8575\n",
            "Test Accuracy with sigmoid activation function: 0.8396\n",
            "\n",
            "Training with tanh activation function:\n",
            "Epoch 1/30 - Training Accuracy: 0.7008518518518518, Validation Accuracy: 0.739\n",
            "Epoch 2/30 - Training Accuracy: 0.7795925925925926, Validation Accuracy: 0.8126666666666666\n",
            "Epoch 3/30 - Training Accuracy: 0.8499814814814814, Validation Accuracy: 0.8808333333333334\n",
            "Epoch 4/30 - Training Accuracy: 0.8779259259259259, Validation Accuracy: 0.9025\n",
            "Epoch 5/30 - Training Accuracy: 0.8941111111111111, Validation Accuracy: 0.9145\n",
            "Epoch 6/30 - Training Accuracy: 0.9037407407407407, Validation Accuracy: 0.919\n",
            "Epoch 7/30 - Training Accuracy: 0.9105925925925926, Validation Accuracy: 0.9238333333333333\n",
            "Epoch 8/30 - Training Accuracy: 0.9167407407407407, Validation Accuracy: 0.9283333333333333\n",
            "Epoch 9/30 - Training Accuracy: 0.9210555555555555, Validation Accuracy: 0.9305\n",
            "Epoch 10/30 - Training Accuracy: 0.9254074074074075, Validation Accuracy: 0.9328333333333333\n",
            "Epoch 11/30 - Training Accuracy: 0.9299259259259259, Validation Accuracy: 0.936\n",
            "Epoch 12/30 - Training Accuracy: 0.9329814814814815, Validation Accuracy: 0.94\n",
            "Epoch 13/30 - Training Accuracy: 0.9362407407407407, Validation Accuracy: 0.9415\n",
            "Epoch 14/30 - Training Accuracy: 0.9388888888888889, Validation Accuracy: 0.9415\n",
            "Epoch 15/30 - Training Accuracy: 0.942037037037037, Validation Accuracy: 0.9453333333333334\n",
            "Epoch 16/30 - Training Accuracy: 0.9444444444444444, Validation Accuracy: 0.9453333333333334\n",
            "Epoch 17/30 - Training Accuracy: 0.9468888888888889, Validation Accuracy: 0.9473333333333334\n",
            "Epoch 18/30 - Training Accuracy: 0.9490555555555555, Validation Accuracy: 0.9475\n",
            "Epoch 19/30 - Training Accuracy: 0.9508703703703704, Validation Accuracy: 0.9495\n",
            "Epoch 20/30 - Training Accuracy: 0.9532037037037037, Validation Accuracy: 0.9513333333333334\n",
            "Epoch 21/30 - Training Accuracy: 0.9552592592592593, Validation Accuracy: 0.9513333333333334\n",
            "Epoch 22/30 - Training Accuracy: 0.9566666666666667, Validation Accuracy: 0.9531666666666667\n",
            "Epoch 23/30 - Training Accuracy: 0.9577592592592593, Validation Accuracy: 0.9531666666666667\n",
            "Epoch 24/30 - Training Accuracy: 0.9598333333333333, Validation Accuracy: 0.9541666666666667\n",
            "Epoch 25/30 - Training Accuracy: 0.9607777777777777, Validation Accuracy: 0.9531666666666667\n",
            "Epoch 26/30 - Training Accuracy: 0.9627037037037037, Validation Accuracy: 0.9545\n",
            "Epoch 27/30 - Training Accuracy: 0.963925925925926, Validation Accuracy: 0.9543333333333334\n",
            "Epoch 28/30 - Training Accuracy: 0.9653703703703703, Validation Accuracy: 0.955\n",
            "Epoch 29/30 - Training Accuracy: 0.9663148148148148, Validation Accuracy: 0.9558333333333333\n",
            "Epoch 30/30 - Training Accuracy: 0.9674259259259259, Validation Accuracy: 0.9575\n",
            "Test Accuracy with tanh activation function: 0.9460\n",
            "\n",
            "Training with relu activation function:\n",
            "Epoch 1/30 - Training Accuracy: 0.9273703703703704, Validation Accuracy: 0.9433333333333334\n",
            "Epoch 2/30 - Training Accuracy: 0.9449444444444445, Validation Accuracy: 0.957\n",
            "Epoch 3/30 - Training Accuracy: 0.955462962962963, Validation Accuracy: 0.9653333333333334\n",
            "Epoch 4/30 - Training Accuracy: 0.9600740740740741, Validation Accuracy: 0.9663333333333334\n",
            "Epoch 5/30 - Training Accuracy: 0.9681666666666666, Validation Accuracy: 0.9705\n",
            "Epoch 6/30 - Training Accuracy: 0.9725555555555555, Validation Accuracy: 0.9743333333333334\n",
            "Epoch 7/30 - Training Accuracy: 0.9760555555555556, Validation Accuracy: 0.9746666666666667\n",
            "Epoch 8/30 - Training Accuracy: 0.9768333333333333, Validation Accuracy: 0.974\n",
            "Epoch 9/30 - Training Accuracy: 0.979462962962963, Validation Accuracy: 0.9776666666666667\n",
            "Epoch 10/30 - Training Accuracy: 0.9804074074074074, Validation Accuracy: 0.9748333333333333\n",
            "Epoch 11/30 - Training Accuracy: 0.9853703703703703, Validation Accuracy: 0.9786666666666667\n",
            "Epoch 12/30 - Training Accuracy: 0.9861481481481481, Validation Accuracy: 0.9788333333333333\n",
            "Epoch 13/30 - Training Accuracy: 0.9886296296296296, Validation Accuracy: 0.981\n",
            "Epoch 14/30 - Training Accuracy: 0.9893518518518518, Validation Accuracy: 0.9818333333333333\n",
            "Epoch 15/30 - Training Accuracy: 0.9900925925925926, Validation Accuracy: 0.9816666666666667\n",
            "Epoch 16/30 - Training Accuracy: 0.9917407407407407, Validation Accuracy: 0.9821666666666666\n",
            "Epoch 17/30 - Training Accuracy: 0.9931666666666666, Validation Accuracy: 0.9823333333333333\n",
            "Epoch 18/30 - Training Accuracy: 0.9932962962962963, Validation Accuracy: 0.9816666666666667\n",
            "Epoch 19/30 - Training Accuracy: 0.9949074074074075, Validation Accuracy: 0.982\n",
            "Epoch 20/30 - Training Accuracy: 0.995037037037037, Validation Accuracy: 0.982\n",
            "Epoch 21/30 - Training Accuracy: 0.9952777777777778, Validation Accuracy: 0.9818333333333333\n",
            "Epoch 22/30 - Training Accuracy: 0.9964814814814815, Validation Accuracy: 0.983\n",
            "Epoch 23/30 - Training Accuracy: 0.9970925925925926, Validation Accuracy: 0.9836666666666667\n",
            "Epoch 24/30 - Training Accuracy: 0.9971851851851852, Validation Accuracy: 0.9818333333333333\n",
            "Epoch 25/30 - Training Accuracy: 0.9960925925925926, Validation Accuracy: 0.982\n",
            "Epoch 26/30 - Training Accuracy: 0.9977592592592592, Validation Accuracy: 0.9838333333333333\n",
            "Epoch 27/30 - Training Accuracy: 0.9979074074074074, Validation Accuracy: 0.9821666666666666\n",
            "Epoch 28/30 - Training Accuracy: 0.9982407407407408, Validation Accuracy: 0.9835\n",
            "Epoch 29/30 - Training Accuracy: 0.9987407407407407, Validation Accuracy: 0.9826666666666667\n",
            "Epoch 30/30 - Training Accuracy: 0.9986481481481482, Validation Accuracy: 0.9835\n",
            "Test Accuracy with relu activation function: 0.9800\n",
            "\n",
            "Training with leaky_relu activation function:\n",
            "Epoch 1/30 - Training Accuracy: 0.9233888888888889, Validation Accuracy: 0.9408333333333333\n",
            "Epoch 2/30 - Training Accuracy: 0.9415370370370371, Validation Accuracy: 0.953\n",
            "Epoch 3/30 - Training Accuracy: 0.9542777777777778, Validation Accuracy: 0.9611666666666666\n",
            "Epoch 4/30 - Training Accuracy: 0.9601481481481482, Validation Accuracy: 0.9651666666666666\n",
            "Epoch 5/30 - Training Accuracy: 0.9661666666666666, Validation Accuracy: 0.9685\n",
            "Epoch 6/30 - Training Accuracy: 0.9709629629629629, Validation Accuracy: 0.971\n",
            "Epoch 7/30 - Training Accuracy: 0.9738703703703704, Validation Accuracy: 0.974\n",
            "Epoch 8/30 - Training Accuracy: 0.9769629629629629, Validation Accuracy: 0.9726666666666667\n",
            "Epoch 9/30 - Training Accuracy: 0.9804444444444445, Validation Accuracy: 0.9758333333333333\n",
            "Epoch 10/30 - Training Accuracy: 0.9822962962962963, Validation Accuracy: 0.9765\n",
            "Epoch 11/30 - Training Accuracy: 0.9835185185185186, Validation Accuracy: 0.9771666666666666\n",
            "Epoch 12/30 - Training Accuracy: 0.9858518518518519, Validation Accuracy: 0.9778333333333333\n",
            "Epoch 13/30 - Training Accuracy: 0.9867407407407407, Validation Accuracy: 0.9773333333333334\n",
            "Epoch 14/30 - Training Accuracy: 0.9874074074074074, Validation Accuracy: 0.9773333333333334\n",
            "Epoch 15/30 - Training Accuracy: 0.9902777777777778, Validation Accuracy: 0.9785\n",
            "Epoch 16/30 - Training Accuracy: 0.9881111111111112, Validation Accuracy: 0.9763333333333334\n",
            "Epoch 17/30 - Training Accuracy: 0.9916481481481482, Validation Accuracy: 0.978\n",
            "Epoch 18/30 - Training Accuracy: 0.9917407407407407, Validation Accuracy: 0.978\n",
            "Epoch 19/30 - Training Accuracy: 0.9935555555555555, Validation Accuracy: 0.9781666666666666\n",
            "Epoch 20/30 - Training Accuracy: 0.9946481481481482, Validation Accuracy: 0.9788333333333333\n",
            "Epoch 21/30 - Training Accuracy: 0.995, Validation Accuracy: 0.9808333333333333\n",
            "Epoch 22/30 - Training Accuracy: 0.9957037037037038, Validation Accuracy: 0.9803333333333333\n",
            "Epoch 23/30 - Training Accuracy: 0.9962222222222222, Validation Accuracy: 0.98\n",
            "Epoch 24/30 - Training Accuracy: 0.9962962962962963, Validation Accuracy: 0.98\n",
            "Epoch 25/30 - Training Accuracy: 0.997, Validation Accuracy: 0.9795\n",
            "Epoch 26/30 - Training Accuracy: 0.9975740740740741, Validation Accuracy: 0.98\n",
            "Epoch 27/30 - Training Accuracy: 0.9979259259259259, Validation Accuracy: 0.9806666666666667\n",
            "Epoch 28/30 - Training Accuracy: 0.9983148148148148, Validation Accuracy: 0.9816666666666667\n",
            "Epoch 29/30 - Training Accuracy: 0.9982407407407408, Validation Accuracy: 0.982\n",
            "Epoch 30/30 - Training Accuracy: 0.998462962962963, Validation Accuracy: 0.9811666666666666\n",
            "Test Accuracy with leaky_relu activation function: 0.9793\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "#Train the network with different activation functions\n",
        "##################################################\n",
        "# Activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x >= 0, x, alpha * x)\n",
        "\n",
        "# Define derivative of activation functions\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (np.full(x.shape, 1) - sigmoid(x))\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return np.full(x.shape, 1) - np.tanh(x)**2\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    return np.where(x > 0, 1, alpha)\n",
        "\n",
        "def fwdPropagate_2(inputs, weights, activ_func):\n",
        "    # Inputs: input data, parameters of network, activation function\n",
        "    W1, W2, W3 = weights\n",
        "\n",
        "    # Forward pass\n",
        "    z1 = np.dot(inputs,W1.T)\n",
        "    a1 = activ_func(z1)\n",
        "    z2 = np.dot(a1, W2.T)\n",
        "    a2 = activ_func(z2)\n",
        "    z3 = np.dot(a2,W3.T)\n",
        "    y = np.array([softmax(i) for i in z3])\n",
        "\n",
        "    return [a1,a2,y,z1,z2]\n",
        "\n",
        "def computeGradients_2(inputs, targets, weights, activations,derivative):\n",
        "    # Inputs: input data, targets, parameters of network, intermediate activations\n",
        "    W1, W2, W3 = weights\n",
        "    a1, a2, y, z1, z2 = activations\n",
        "\n",
        "    # One-hot encode the targets\n",
        "    targets_onehot = np.eye(10)[targets]\n",
        "\n",
        "\n",
        "\n",
        "    # Compute loss\n",
        "    # loss = -np.sum(targets_onehot @ np.log(out puts.T)) / len(targets)\n",
        "\n",
        "    # Compute gradients\n",
        "    delta3 = (y - targets_onehot)\n",
        "    delta2 = np.dot(delta3, W3) * derivative(z2)\n",
        "    delta1 = np.dot(delta2, W2) * derivative(z1)\n",
        "\n",
        "    dj_dw3 = np.dot(delta3.T, a2)\n",
        "    dj_dw2 = np.dot(delta2.T, a1)\n",
        "    dj_dw1 = np.dot(delta1.T, inputs)\n",
        "\n",
        "    # Return the gradients and loss\n",
        "    return [dj_dw1, dj_dw2, dj_dw3]\n",
        "\n",
        "\n",
        "def backPropagate_2(inputs, targets, weights, activations, learning_rate,derivative):\n",
        "    # Compute gradients\n",
        "    gradients = computeGradients_2(inputs, targets, weights, activations,derivative)\n",
        "\n",
        "    # Update weights\n",
        "    weights = applyGradients(weights, gradients, learning_rate)\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "# Define training function\n",
        "# Training function\n",
        "def train_with_activation(inputs, targets_idx, activ_func, derivative, batch_size=300, epochs=30, train_val_split=0.9, learning_rate=0.00001):\n",
        "    hidden_units = 512\n",
        "    n_classes = 10\n",
        "    n_samples = inputs.shape[0]\n",
        "\n",
        "    # Split the training data into training and validation sets\n",
        "    split_idx = int(n_samples * train_val_split)\n",
        "    inputs_train, inputs_val = inputs[:split_idx], inputs[split_idx:]\n",
        "    targets_train, targets_val = targets_idx[:split_idx], targets_idx[split_idx:]\n",
        "\n",
        "    n_batches = split_idx // batch_size\n",
        "\n",
        "    # Randomly initialize the weights\n",
        "    W1 = np.random.randn(hidden_units, inputs.shape[1]) * 0.01\n",
        "    W2 = np.random.randn(hidden_units, hidden_units) * 0.01\n",
        "    W3 = np.random.randn(n_classes, hidden_units) * 0.01\n",
        "    weights = [W1, W2, W3]\n",
        "\n",
        "    # Iterate for epochs times\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle the training data\n",
        "        indices = np.random.permutation(split_idx)\n",
        "        inputs_train_shuffled = inputs_train[indices]\n",
        "        targets_train_shuffled = targets_train[indices]\n",
        "\n",
        "        # Iterate through the batches of data\n",
        "        for batch in range(n_batches):\n",
        "            # Get the batch of data\n",
        "            start = batch * batch_size\n",
        "            end = min((batch + 1) * batch_size, split_idx)\n",
        "            inputs_batch = inputs_train_shuffled[start:end]\n",
        "            targets_batch = targets_train_shuffled[start:end]\n",
        "\n",
        "            # Forward propagation\n",
        "            activations = fwdPropagate_2(inputs_batch, weights, activ_func)\n",
        "\n",
        "            # Backward propagation\n",
        "            weights = backPropagate_2(inputs_batch, targets_batch, weights, activations, learning_rate, derivative)\n",
        "\n",
        "        # Compute outputs on training data\n",
        "        _, _, outputs_train, _, _ = fwdPropagate_2(inputs_train, weights, activ_func)\n",
        "        predicted_labels_train = np.argmax(outputs_train, axis=1)\n",
        "        training_accuracy = np.mean(predicted_labels_train == targets_train)\n",
        "\n",
        "        # Compute outputs on validation data\n",
        "        _, _, outputs_val, _, _ = fwdPropagate_2(inputs_val, weights, activ_func)\n",
        "        predicted_labels_val = np.argmax(outputs_val, axis=1)\n",
        "        validation_accuracy = np.mean(predicted_labels_val == targets_val)\n",
        "\n",
        "        # Print statistics\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Training Accuracy: {training_accuracy}, Validation Accuracy: {validation_accuracy}\")\n",
        "\n",
        "    return weights\n",
        "\n",
        "def evaluate_2(inputs, targets_idx, weights, activ_func):\n",
        "    # Forward pass\n",
        "    _, _, outputs, _, _ = fwdPropagate_2(inputs, weights,activ_func)\n",
        "    \n",
        "    # Assign inputs to the class having maximum posterior probability\n",
        "    preds = np.argmax(outputs, axis=1)\n",
        "    \n",
        "    # Compute accuracy\n",
        "    accuracy = np.mean(preds == targets_idx)\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Activation function dictionary\n",
        "activation_functions = {\n",
        "    'sigmoid': (sigmoid, sigmoid_derivative),\n",
        "    'tanh': (tanh, tanh_derivative),\n",
        "    'relu': (relu, relu_derivative),\n",
        "    'leaky_relu': (leaky_relu, leaky_relu_derivative)\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Evaluate the performance on MNIST test data\n",
        "##################################################\n",
        "# Evaluate performance on MNIST test data\n",
        "for activation_name, (activation, derivative) in activation_functions.items():\n",
        "    print(f\"\\nTraining with {activation_name} activation function:\")\n",
        "    trained_weights = train_with_activation(mnist_traindata.reshape(-1, 784), mnist_trainlabel, activation, derivative)\n",
        "    test_accuracy = evaluate_2(mnist_testdata.reshape(-1, 784), mnist_testlabel, trained_weights, activation)\n",
        "    print(f\"Test Accuracy with {activation_name} activation function: {test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwXhr-KaU7V2"
      },
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1. Tanh performs better than sigmoid due to its zero-centered nature, which helps in faster convergence.\n",
        "\n",
        "2. ReLU shows rapid convergence and achieves high training and validation accuracy early on. It achieves a test accuracy of 98.00%.\n",
        "\n",
        "3. Similar to ReLU, leaky ReLU shows rapid convergence and achieves high accuracy.\n",
        "It achieves a test accuracy of 97.93%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENZXVRQ9QsgV"
      },
      "source": [
        "<b> Part - (3) : Understanding optimization algorithms: </b> In this part you will learn to use different optimiztion algorithm apart from SGD.\n",
        "\n",
        "<dt> <h6> 1. Using the best activation function from Part - (2), train the classification network using Adam optimization algorithm. </dt> </h6>\n",
        "<dt> <h6> 3. Compare the accuracy of the networks trained with SGD and Adam optimization algorithms. </dt> </h6>\n",
        "<dt> <h6> 4. Report your observations. </dt> </h6>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1gzX3-rlVKZC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training with Adam optimization algorithm:\n",
            "Epoch 1/30 - Training Accuracy: 0.9676851851851852, Validation Accuracy: 0.9643333333333334\n",
            "Epoch 2/30 - Training Accuracy: 0.9823148148148149, Validation Accuracy: 0.971\n",
            "Epoch 3/30 - Training Accuracy: 0.9827592592592592, Validation Accuracy: 0.9718333333333333\n",
            "Epoch 4/30 - Training Accuracy: 0.986574074074074, Validation Accuracy: 0.9748333333333333\n",
            "Epoch 5/30 - Training Accuracy: 0.9886296296296296, Validation Accuracy: 0.9765\n",
            "Epoch 6/30 - Training Accuracy: 0.9911296296296296, Validation Accuracy: 0.9791666666666666\n",
            "Epoch 7/30 - Training Accuracy: 0.9923518518518518, Validation Accuracy: 0.9806666666666667\n",
            "Epoch 8/30 - Training Accuracy: 0.9892222222222222, Validation Accuracy: 0.9796666666666667\n",
            "Epoch 9/30 - Training Accuracy: 0.9895, Validation Accuracy: 0.9758333333333333\n",
            "Epoch 10/30 - Training Accuracy: 0.9906481481481482, Validation Accuracy: 0.9778333333333333\n",
            "Epoch 11/30 - Training Accuracy: 0.993, Validation Accuracy: 0.9806666666666667\n",
            "Epoch 12/30 - Training Accuracy: 0.9896481481481482, Validation Accuracy: 0.9758333333333333\n",
            "Epoch 13/30 - Training Accuracy: 0.9951296296296296, Validation Accuracy: 0.9808333333333333\n",
            "Epoch 14/30 - Training Accuracy: 0.9942777777777778, Validation Accuracy: 0.9796666666666667\n",
            "Epoch 15/30 - Training Accuracy: 0.9931666666666666, Validation Accuracy: 0.9786666666666667\n",
            "Epoch 16/30 - Training Accuracy: 0.9922592592592593, Validation Accuracy: 0.9796666666666667\n",
            "Epoch 17/30 - Training Accuracy: 0.9927037037037038, Validation Accuracy: 0.9775\n",
            "Epoch 18/30 - Training Accuracy: 0.9956666666666667, Validation Accuracy: 0.9811666666666666\n",
            "Epoch 19/30 - Training Accuracy: 0.9942407407407408, Validation Accuracy: 0.9813333333333333\n",
            "Epoch 20/30 - Training Accuracy: 0.994462962962963, Validation Accuracy: 0.98\n",
            "Epoch 21/30 - Training Accuracy: 0.9922222222222222, Validation Accuracy: 0.9768333333333333\n",
            "Epoch 22/30 - Training Accuracy: 0.9941111111111111, Validation Accuracy: 0.9781666666666666\n",
            "Epoch 23/30 - Training Accuracy: 0.995037037037037, Validation Accuracy: 0.9796666666666667\n",
            "Epoch 24/30 - Training Accuracy: 0.9956851851851852, Validation Accuracy: 0.9808333333333333\n",
            "Epoch 25/30 - Training Accuracy: 0.9950185185185185, Validation Accuracy: 0.979\n",
            "Epoch 26/30 - Training Accuracy: 0.9949629629629629, Validation Accuracy: 0.9805\n",
            "Epoch 27/30 - Training Accuracy: 0.9968703703703704, Validation Accuracy: 0.981\n",
            "Epoch 28/30 - Training Accuracy: 0.9958148148148148, Validation Accuracy: 0.9766666666666667\n",
            "Epoch 29/30 - Training Accuracy: 0.9966851851851852, Validation Accuracy: 0.9815\n",
            "Epoch 30/30 - Training Accuracy: 0.9962037037037037, Validation Accuracy: 0.9795\n",
            "Test Accuracy with Adam optimization algorithm: 0.9783\n",
            "\n",
            "SGD Test Accuracy: 0.9793\n",
            "Adam Test Accuracy: 0.9783\n"
          ]
        }
      ],
      "source": [
        "def adamOptimizer(inputs, targets_idx, batch_size=300, epochs=30, train_val_split=0.9, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    hidden_units = 512\n",
        "    n_classes = 10\n",
        "    n_samples = inputs.shape[0]\n",
        "\n",
        "    # Split the training data into training and validation sets\n",
        "    split_idx = int(n_samples * train_val_split)\n",
        "    inputs_train, inputs_val = inputs[:split_idx], inputs[split_idx:]\n",
        "    targets_train, targets_val = targets_idx[:split_idx], targets_idx[split_idx:]\n",
        "\n",
        "    n_batches = split_idx // batch_size\n",
        "\n",
        "    # Randomly initialize the weights\n",
        "    W1 = np.random.randn(hidden_units, inputs.shape[1]) * 0.01\n",
        "    W2 = np.random.randn(hidden_units, hidden_units) * 0.01\n",
        "    W3 = np.random.randn(n_classes, hidden_units) * 0.01\n",
        "    weights = [W1, W2, W3]\n",
        "\n",
        "    # Initialize moment estimates\n",
        "    m_W1 = np.zeros_like(W1)\n",
        "    m_W2 = np.zeros_like(W2)\n",
        "    m_W3 = np.zeros_like(W3)\n",
        "\n",
        "    v_W1 = np.zeros_like(W1)\n",
        "    v_W2 = np.zeros_like(W2)\n",
        "    v_W3 = np.zeros_like(W3)\n",
        "\n",
        "    # Iterate for epochs times\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle the training data\n",
        "        indices = np.random.permutation(split_idx)\n",
        "        inputs_train_shuffled = inputs_train[indices]\n",
        "        targets_train_shuffled = targets_train[indices]\n",
        "\n",
        "        # Iterate through the batches of data\n",
        "        for batch in range(n_batches):\n",
        "            # Get the batch of data\n",
        "            start = batch * batch_size\n",
        "            end = min((batch + 1) * batch_size, split_idx)\n",
        "            inputs_batch = inputs_train_shuffled[start:end]\n",
        "            targets_batch = targets_train_shuffled[start:end]\n",
        "\n",
        "            # Forward propagation\n",
        "            activations = fwdPropagate_2(inputs_batch, weights, relu)\n",
        "\n",
        "            # Backward propagation\n",
        "            gradients = computeGradients_2(inputs_batch, targets_batch, weights, activations, relu_derivative)\n",
        "            \n",
        "            # Update time step\n",
        "            t = epoch * n_batches + batch + 1\n",
        "\n",
        "            # Update moment estimates\n",
        "            m_W1 = beta1 * m_W1 + (1 - beta1) * gradients[0]\n",
        "            m_W2 = beta1 * m_W2 + (1 - beta1) * gradients[1]\n",
        "            m_W3 = beta1 * m_W3 + (1 - beta1) * gradients[2]\n",
        "\n",
        "            v_W1 = beta2 * v_W1 + (1 - beta2) * (gradients[0] ** 2)\n",
        "            v_W2 = beta2 * v_W2 + (1 - beta2) * (gradients[1] ** 2)\n",
        "            v_W3 = beta2 * v_W3 + (1 - beta2) * (gradients[2] ** 2)\n",
        "\n",
        "            # Bias correction\n",
        "            m_W1_corr = m_W1 / (1 - beta1 ** t)\n",
        "            m_W2_corr = m_W2 / (1 - beta1 ** t)\n",
        "            m_W3_corr = m_W3 / (1 - beta1 ** t)\n",
        "\n",
        "            v_W1_corr = v_W1 / (1 - beta2 ** t)\n",
        "            v_W2_corr = v_W2 / (1 - beta2 ** t)\n",
        "            v_W3_corr = v_W3 / (1 - beta2 ** t)\n",
        "\n",
        "            # Update weights\n",
        "            weights[0] -= learning_rate * m_W1_corr / (np.sqrt(v_W1_corr) + epsilon)\n",
        "            weights[1] -= learning_rate * m_W2_corr / (np.sqrt(v_W2_corr) + epsilon)\n",
        "            weights[2] -= learning_rate * m_W3_corr / (np.sqrt(v_W3_corr) + epsilon)\n",
        "\n",
        "        # Compute outputs on training data\n",
        "        _, _, outputs_train, _, _ = fwdPropagate_2(inputs_train, weights, relu)\n",
        "        predicted_labels_train = np.argmax(outputs_train, axis=1)\n",
        "        training_accuracy = np.mean(predicted_labels_train == targets_train)\n",
        "\n",
        "        # Compute outputs on validation data\n",
        "        _, _, outputs_val, _, _ = fwdPropagate_2(inputs_val, weights, relu)\n",
        "        predicted_labels_val = np.argmax(outputs_val, axis=1)\n",
        "        validation_accuracy = np.mean(predicted_labels_val == targets_val)\n",
        "\n",
        "        # Print statistics\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Training Accuracy: {training_accuracy}, Validation Accuracy: {validation_accuracy}\")\n",
        "\n",
        "    return weights\n",
        "\n",
        "# Train the network with Adam optimization algorithm\n",
        "print(\"\\nTraining with Adam optimization algorithm:\")\n",
        "trained_weights_adam = adamOptimizer(mnist_traindata.reshape(-1, 784), mnist_trainlabel)\n",
        "\n",
        "# Evaluate performance on test data\n",
        "test_accuracy_adam = evaluate_2(mnist_testdata.reshape(-1, 784), mnist_testlabel, trained_weights_adam, relu)\n",
        "print(f\"Test Accuracy with Adam optimization algorithm: {test_accuracy_adam:.4f}\")\n",
        "\n",
        "# Compare the accuracy of networks trained with SGD and Adam optimization algorithms\n",
        "print(f\"\\nSGD Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Adam Test Accuracy: {test_accuracy_adam:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mYI0MNpVgnk"
      },
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1. The Adam optimization algorithm shows improvement in training and validation accuracies over epochs.\n",
        "It achieves a test accuracy of 0.9783, which is slightly lower than the test accuracy achieved using SGD (0.9793). However, the difference is minimal.\n",
        "\n",
        "2. Final Test Accuracies:\n",
        "\n",
        "Sigmoid: Test Accuracy = 0.8396\n",
        "Tanh: Test Accuracy = 0.9460\n",
        "ReLU: Test Accuracy = 0.9800\n",
        "Leaky ReLU: Test Accuracy = 0.9793....\n",
        "The ReLU activation function achieves the highest test accuracy among the tested activation functions, followed closely by Leaky ReLU. Tanh performs reasonably well, while Sigmoid lags behind the others.\n",
        "\n",
        "3.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FXj11mJO_Dd"
      },
      "source": [
        "<b> Part - (4) : Understanding regularization methods: </b> In this part of the assignment, you will learn about a few regularization techniques to reduce the overfitting problem. Using the above built network, include the following techniques to reduce the overfitting by retraining the network efficiently. Write down the accuracies for each case.\n",
        "<dt> <h6> 1. Weight regularization: Add regularization term to the classification los </dt> </h6>\n",
        "<dt> <h6> 2. Dropout with a probability of 0.2: Randomly drop the activation potentials of hidden neural with 0.2 probability. Disable the dropout layer in inference model. You can experiment with different dropout probabilities and report your observations.  </dt> </h6>\n",
        "<dt> <h6> 3. Early stopping: Stop the network training when it is started to overfitting to training data. </dt> </h6>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jbXfF_W4Vwyy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training with weight regularization:\n",
            "Epoch 1/30 - Training Accuracy: 0.9226666666666666, Validation Accuracy: 0.9378333333333333\n",
            "Epoch 2/30 - Training Accuracy: 0.940962962962963, Validation Accuracy: 0.9535\n",
            "Epoch 3/30 - Training Accuracy: 0.9522962962962963, Validation Accuracy: 0.9581666666666667\n",
            "Epoch 4/30 - Training Accuracy: 0.960925925925926, Validation Accuracy: 0.9641666666666666\n",
            "Epoch 5/30 - Training Accuracy: 0.9663888888888889, Validation Accuracy: 0.9686666666666667\n",
            "Epoch 6/30 - Training Accuracy: 0.9709444444444445, Validation Accuracy: 0.9701666666666666\n",
            "Epoch 7/30 - Training Accuracy: 0.9738518518518519, Validation Accuracy: 0.9698333333333333\n",
            "Epoch 8/30 - Training Accuracy: 0.9776111111111111, Validation Accuracy: 0.9725\n",
            "Epoch 9/30 - Training Accuracy: 0.9809444444444444, Validation Accuracy: 0.9735\n",
            "Epoch 10/30 - Training Accuracy: 0.9821666666666666, Validation Accuracy: 0.9735\n",
            "Epoch 11/30 - Training Accuracy: 0.9842037037037037, Validation Accuracy: 0.9758333333333333\n",
            "Epoch 12/30 - Training Accuracy: 0.9857592592592592, Validation Accuracy: 0.9745\n",
            "Epoch 13/30 - Training Accuracy: 0.9888518518518519, Validation Accuracy: 0.9765\n",
            "Epoch 14/30 - Training Accuracy: 0.9894074074074074, Validation Accuracy: 0.9763333333333334\n",
            "Epoch 15/30 - Training Accuracy: 0.9893333333333333, Validation Accuracy: 0.9763333333333334\n",
            "Epoch 16/30 - Training Accuracy: 0.9905925925925926, Validation Accuracy: 0.9765\n",
            "Epoch 17/30 - Training Accuracy: 0.9907037037037038, Validation Accuracy: 0.9773333333333334\n",
            "Epoch 18/30 - Training Accuracy: 0.9935, Validation Accuracy: 0.978\n",
            "Epoch 19/30 - Training Accuracy: 0.9937777777777778, Validation Accuracy: 0.9781666666666666\n",
            "Epoch 20/30 - Training Accuracy: 0.9947407407407407, Validation Accuracy: 0.9776666666666667\n",
            "Epoch 21/30 - Training Accuracy: 0.9939444444444444, Validation Accuracy: 0.9775\n",
            "Epoch 22/30 - Training Accuracy: 0.9955185185185185, Validation Accuracy: 0.979\n",
            "Epoch 23/30 - Training Accuracy: 0.9951666666666666, Validation Accuracy: 0.9781666666666666\n",
            "Epoch 24/30 - Training Accuracy: 0.9963703703703704, Validation Accuracy: 0.9786666666666667\n",
            "Epoch 25/30 - Training Accuracy: 0.9971851851851852, Validation Accuracy: 0.9781666666666666\n",
            "Epoch 26/30 - Training Accuracy: 0.9979814814814815, Validation Accuracy: 0.9783333333333334\n",
            "Epoch 27/30 - Training Accuracy: 0.9981481481481481, Validation Accuracy: 0.9791666666666666\n",
            "Epoch 28/30 - Training Accuracy: 0.9982037037037037, Validation Accuracy: 0.9791666666666666\n",
            "Epoch 29/30 - Training Accuracy: 0.9985, Validation Accuracy: 0.979\n",
            "Epoch 30/30 - Training Accuracy: 0.9987777777777778, Validation Accuracy: 0.9798333333333333\n",
            "Test Accuracy with weight regularization: 0.9795\n",
            "\n",
            "Training with dropout strategy:\n",
            "Epoch 1/30 - Training Accuracy: 0.9161851851851852, Validation Accuracy: 0.9306666666666666\n",
            "Epoch 2/30 - Training Accuracy: 0.9358703703703704, Validation Accuracy: 0.948\n",
            "Epoch 3/30 - Training Accuracy: 0.9447407407407408, Validation Accuracy: 0.9565\n",
            "Epoch 4/30 - Training Accuracy: 0.9555185185185185, Validation Accuracy: 0.962\n",
            "Epoch 5/30 - Training Accuracy: 0.9617407407407408, Validation Accuracy: 0.968\n",
            "Epoch 6/30 - Training Accuracy: 0.9651851851851851, Validation Accuracy: 0.9688333333333333\n",
            "Epoch 7/30 - Training Accuracy: 0.9691111111111111, Validation Accuracy: 0.9706666666666667\n",
            "Epoch 8/30 - Training Accuracy: 0.9710740740740741, Validation Accuracy: 0.9716666666666667\n",
            "Epoch 9/30 - Training Accuracy: 0.9736296296296296, Validation Accuracy: 0.9721666666666666\n",
            "Epoch 10/30 - Training Accuracy: 0.9774074074074074, Validation Accuracy: 0.9748333333333333\n",
            "Epoch 11/30 - Training Accuracy: 0.9795555555555555, Validation Accuracy: 0.9751666666666666\n",
            "Epoch 12/30 - Training Accuracy: 0.9789814814814815, Validation Accuracy: 0.973\n",
            "Epoch 13/30 - Training Accuracy: 0.9817222222222223, Validation Accuracy: 0.975\n",
            "Epoch 14/30 - Training Accuracy: 0.984462962962963, Validation Accuracy: 0.976\n",
            "Epoch 15/30 - Training Accuracy: 0.9854444444444445, Validation Accuracy: 0.9775\n",
            "Epoch 16/30 - Training Accuracy: 0.9858703703703704, Validation Accuracy: 0.9753333333333334\n",
            "Epoch 17/30 - Training Accuracy: 0.9874814814814815, Validation Accuracy: 0.9776666666666667\n",
            "Epoch 18/30 - Training Accuracy: 0.9880925925925926, Validation Accuracy: 0.9778333333333333\n",
            "Epoch 19/30 - Training Accuracy: 0.9888888888888889, Validation Accuracy: 0.977\n",
            "Epoch 20/30 - Training Accuracy: 0.9899444444444444, Validation Accuracy: 0.9778333333333333\n",
            "Epoch 21/30 - Training Accuracy: 0.9914074074074074, Validation Accuracy: 0.9783333333333334\n",
            "Epoch 22/30 - Training Accuracy: 0.9921666666666666, Validation Accuracy: 0.9788333333333333\n",
            "Epoch 23/30 - Training Accuracy: 0.9918148148148148, Validation Accuracy: 0.9786666666666667\n",
            "Epoch 24/30 - Training Accuracy: 0.9929629629629629, Validation Accuracy: 0.9796666666666667\n",
            "Epoch 25/30 - Training Accuracy: 0.9939814814814815, Validation Accuracy: 0.9783333333333334\n",
            "Epoch 26/30 - Training Accuracy: 0.9938148148148148, Validation Accuracy: 0.9801666666666666\n",
            "Epoch 27/30 - Training Accuracy: 0.9949814814814815, Validation Accuracy: 0.9796666666666667\n",
            "Epoch 28/30 - Training Accuracy: 0.9955, Validation Accuracy: 0.9798333333333333\n",
            "Epoch 29/30 - Training Accuracy: 0.9960185185185185, Validation Accuracy: 0.9803333333333333\n",
            "Epoch 30/30 - Training Accuracy: 0.9965, Validation Accuracy: 0.9793333333333333\n",
            "Test Accuracy with dropout strategy: 0.9790\n",
            "\n",
            "Training with early stopping criterion:\n",
            "Epoch 1/30 - Training Accuracy: 0.9227407407407408, Validation Accuracy: 0.9405\n",
            "Epoch 2/30 - Training Accuracy: 0.9416666666666667, Validation Accuracy: 0.9525\n",
            "Epoch 3/30 - Training Accuracy: 0.954925925925926, Validation Accuracy: 0.963\n",
            "Epoch 4/30 - Training Accuracy: 0.9616111111111111, Validation Accuracy: 0.9673333333333334\n",
            "Epoch 5/30 - Training Accuracy: 0.9674444444444444, Validation Accuracy: 0.9678333333333333\n",
            "Epoch 6/30 - Training Accuracy: 0.9723333333333334, Validation Accuracy: 0.9723333333333334\n",
            "Epoch 7/30 - Training Accuracy: 0.9741666666666666, Validation Accuracy: 0.9726666666666667\n",
            "Epoch 8/30 - Training Accuracy: 0.9790740740740741, Validation Accuracy: 0.975\n",
            "Epoch 9/30 - Training Accuracy: 0.9804259259259259, Validation Accuracy: 0.9748333333333333\n",
            "Epoch 10/30 - Training Accuracy: 0.9834074074074074, Validation Accuracy: 0.9761666666666666\n",
            "Epoch 11/30 - Training Accuracy: 0.9851666666666666, Validation Accuracy: 0.9773333333333334\n",
            "Epoch 12/30 - Training Accuracy: 0.986, Validation Accuracy: 0.9768333333333333\n",
            "Epoch 13/30 - Training Accuracy: 0.9861481481481481, Validation Accuracy: 0.9765\n",
            "Epoch 14/30 - Training Accuracy: 0.9896481481481482, Validation Accuracy: 0.9793333333333333\n",
            "Epoch 15/30 - Training Accuracy: 0.9899629629629629, Validation Accuracy: 0.9783333333333334\n",
            "Epoch 16/30 - Training Accuracy: 0.9913333333333333, Validation Accuracy: 0.9781666666666666\n",
            "Epoch 17/30 - Training Accuracy: 0.9921111111111112, Validation Accuracy: 0.979\n",
            "Epoch 18/30 - Training Accuracy: 0.9926111111111111, Validation Accuracy: 0.9801666666666666\n",
            "Epoch 19/30 - Training Accuracy: 0.9939259259259259, Validation Accuracy: 0.9781666666666666\n",
            "Epoch 20/30 - Training Accuracy: 0.9951296296296296, Validation Accuracy: 0.9811666666666666\n",
            "Epoch 21/30 - Training Accuracy: 0.9957962962962963, Validation Accuracy: 0.981\n",
            "Epoch 22/30 - Training Accuracy: 0.9957592592592592, Validation Accuracy: 0.98\n",
            "Epoch 23/30 - Training Accuracy: 0.9965555555555555, Validation Accuracy: 0.9798333333333333\n",
            "Epoch 24/30 - Training Accuracy: 0.997037037037037, Validation Accuracy: 0.9808333333333333\n",
            "Epoch 25/30 - Training Accuracy: 0.9972592592592593, Validation Accuracy: 0.9811666666666666\n",
            "Early stopping triggered!\n",
            "Test Accuracy with early stopping criterion: 0.9787\n"
          ]
        }
      ],
      "source": [
        "##################################################\n",
        "#Training with weight regularization\n",
        "##################################################\n",
        "def train_with_weight_regularization(inputs, targets_idx, activ_func, derivative, lambda_reg=0.01, batch_size=300, epochs=30, train_val_split=0.9, learning_rate=0.00001):\n",
        "    hidden_units = 512\n",
        "    n_classes = 10\n",
        "    n_samples = inputs.shape[0]\n",
        "\n",
        "    # Split the training data into training and validation sets\n",
        "    split_idx = int(n_samples * train_val_split)\n",
        "    inputs_train, inputs_val = inputs[:split_idx], inputs[split_idx:]\n",
        "    targets_train, targets_val = targets_idx[:split_idx], targets_idx[split_idx:]\n",
        "\n",
        "    n_batches = split_idx // batch_size\n",
        "\n",
        "    # Randomly initialize the weights\n",
        "    W1 = np.random.randn(hidden_units, inputs.shape[1]) * 0.01\n",
        "    W2 = np.random.randn(hidden_units, hidden_units) * 0.01\n",
        "    W3 = np.random.randn(n_classes, hidden_units) * 0.01\n",
        "    weights = [W1, W2, W3]\n",
        "\n",
        "    # Iterate for epochs times\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle the training data\n",
        "        indices = np.random.permutation(split_idx)\n",
        "        inputs_train_shuffled = inputs_train[indices]\n",
        "        targets_train_shuffled = targets_train[indices]\n",
        "\n",
        "        # Iterate through the batches of data\n",
        "        for batch in range(n_batches):\n",
        "            # Get the batch of data\n",
        "            start = batch * batch_size\n",
        "            end = min((batch + 1) * batch_size, split_idx)\n",
        "            inputs_batch = inputs_train_shuffled[start:end]\n",
        "            targets_batch = targets_train_shuffled[start:end]\n",
        "\n",
        "            # Forward propagation\n",
        "            activations = fwdPropagate_2(inputs_batch, weights, activ_func)\n",
        "\n",
        "            # Backward propagation with weight regularization\n",
        "            gradients = computeGradients_2(inputs_batch, targets_batch, weights, activations, derivative)\n",
        "            for i in range(len(gradients)):\n",
        "                gradients[i] += lambda_reg * weights[i]\n",
        "\n",
        "            # Update weights\n",
        "            weights = backPropagate_2(inputs_batch, targets_batch, weights, activations, learning_rate, derivative)\n",
        "\n",
        "        # Compute outputs on training data\n",
        "        _, _, outputs_train, _, _ = fwdPropagate_2(inputs_train, weights, activ_func)\n",
        "        predicted_labels_train = np.argmax(outputs_train, axis=1)\n",
        "        training_accuracy = np.mean(predicted_labels_train == targets_train)\n",
        "\n",
        "        # Compute outputs on validation data\n",
        "        _, _, outputs_val, _, _ = fwdPropagate_2(inputs_val, weights, activ_func)\n",
        "        predicted_labels_val = np.argmax(outputs_val, axis=1)\n",
        "        validation_accuracy = np.mean(predicted_labels_val == targets_val)\n",
        "\n",
        "        # Print statistics\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Training Accuracy: {training_accuracy}, Validation Accuracy: {validation_accuracy}\")\n",
        "\n",
        "    return weights\n",
        "\n",
        "# Train the network with weight regularization\n",
        "print(\"\\nTraining with weight regularization:\")\n",
        "trained_weights_reg = train_with_weight_regularization(mnist_traindata.reshape(-1, 784), mnist_trainlabel, relu, relu_derivative)\n",
        "test_accuracy_reg = evaluate_2(mnist_testdata.reshape(-1, 784), mnist_testlabel, trained_weights_reg, relu)\n",
        "print(f\"Test Accuracy with weight regularization: {test_accuracy_reg:.4f}\")\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Training with dropout strategy\n",
        "##################################################\n",
        "def train_with_dropout(inputs, targets_idx, activ_func, derivative, dropout_prob=0.2, batch_size=300, epochs=30, train_val_split=0.9, learning_rate=0.00001):\n",
        "    hidden_units = 512\n",
        "    n_classes = 10\n",
        "    n_samples = inputs.shape[0]\n",
        "\n",
        "    # Split the training data into training and validation sets\n",
        "    split_idx = int(n_samples * train_val_split)\n",
        "    inputs_train, inputs_val = inputs[:split_idx], inputs[split_idx:]\n",
        "    targets_train, targets_val = targets_idx[:split_idx], targets_idx[split_idx:]\n",
        "\n",
        "    n_batches = split_idx // batch_size\n",
        "\n",
        "    # Randomly initialize the weights\n",
        "    W1 = np.random.randn(hidden_units, inputs.shape[1]) * 0.01\n",
        "    W2 = np.random.randn(hidden_units, hidden_units) * 0.01\n",
        "    W3 = np.random.randn(n_classes, hidden_units) * 0.01\n",
        "    weights = [W1, W2, W3]\n",
        "\n",
        "    # Iterate for epochs times\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle the training data\n",
        "        indices = np.random.permutation(split_idx)\n",
        "        inputs_train_shuffled = inputs_train[indices]\n",
        "        targets_train_shuffled = targets_train[indices]\n",
        "\n",
        "        # Iterate through the batches of data\n",
        "        for batch in range(n_batches):\n",
        "            # Get the batch of data\n",
        "            start = batch * batch_size\n",
        "            end = min((batch + 1) * batch_size, split_idx)\n",
        "            inputs_batch = inputs_train_shuffled[start:end]\n",
        "            targets_batch = targets_train_shuffled[start:end]\n",
        "\n",
        "            # Forward propagation\n",
        "            activations = fwdPropagate_2(inputs_batch, weights, activ_func)\n",
        "\n",
        "            # Apply dropout to hidden layers\n",
        "            dropout_mask = np.random.rand(*activations[0].shape) < dropout_prob\n",
        "            activations[0] *= dropout_mask\n",
        "\n",
        "            # Backward propagation\n",
        "            weights = backPropagate_2(inputs_batch, targets_batch, weights, activations, learning_rate, derivative)\n",
        "\n",
        "        # Compute outputs on training data\n",
        "        _, _, outputs_train, _, _ = fwdPropagate_2(inputs_train, weights, activ_func)\n",
        "        predicted_labels_train = np.argmax(outputs_train, axis=1)\n",
        "        training_accuracy = np.mean(predicted_labels_train == targets_train)\n",
        "\n",
        "        # Compute outputs on validation data\n",
        "        _, _, outputs_val, _, _ = fwdPropagate_2(inputs_val, weights, activ_func)\n",
        "        predicted_labels_val = np.argmax(outputs_val, axis=1)\n",
        "        validation_accuracy = np.mean(predicted_labels_val == targets_val)\n",
        "\n",
        "        # Print statistics\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Training Accuracy: {training_accuracy}, Validation Accuracy: {validation_accuracy}\")\n",
        "\n",
        "    return weights\n",
        "\n",
        "# Train the network with dropout\n",
        "print(\"\\nTraining with dropout strategy:\")\n",
        "trained_weights_dropout = train_with_dropout(mnist_traindata.reshape(-1, 784), mnist_trainlabel, relu, relu_derivative)\n",
        "test_accuracy_dropout = evaluate_2(mnist_testdata.reshape(-1, 784), mnist_testlabel, trained_weights_dropout, relu)\n",
        "print(f\"Test Accuracy with dropout strategy: {test_accuracy_dropout:.4f}\")\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Training with early stopping criterion\n",
        "##################################################\n",
        "def train_with_early_stopping(inputs, targets_idx, activ_func, derivative, limit=5, batch_size=300, epochs=30, train_val_split=0.9, learning_rate=0.00001):\n",
        "    hidden_units = 512\n",
        "    n_classes = 10\n",
        "    n_samples = inputs.shape[0]\n",
        "\n",
        "    # Split the training data into training and validation sets\n",
        "    split_idx = int(n_samples * train_val_split)\n",
        "    inputs_train, inputs_val = inputs[:split_idx], inputs[split_idx:]\n",
        "    targets_train, targets_val = targets_idx[:split_idx], targets_idx[split_idx:]\n",
        "\n",
        "    n_batches = split_idx // batch_size\n",
        "\n",
        "    # Randomly initialize the weights\n",
        "    W1 = np.random.randn(hidden_units, inputs.shape[1]) * 0.01\n",
        "    W2 = np.random.randn(hidden_units, hidden_units) * 0.01\n",
        "    W3 = np.random.randn(n_classes, hidden_units) * 0.01\n",
        "    weights = [W1, W2, W3]\n",
        "\n",
        "    best_validation_accuracy = 0\n",
        "    best_weights = weights.copy()\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    # Iterate for epochs times\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle the training data\n",
        "        indices = np.random.permutation(split_idx)\n",
        "        inputs_train_shuffled = inputs_train[indices]\n",
        "        targets_train_shuffled = targets_train[indices]\n",
        "\n",
        "        # Iterate through the batches of data\n",
        "        for batch in range(n_batches):\n",
        "            # Get the batch of data\n",
        "            start = batch * batch_size\n",
        "            end = min((batch + 1) * batch_size, split_idx)\n",
        "            inputs_batch = inputs_train_shuffled[start:end]\n",
        "            targets_batch = targets_train_shuffled[start:end]\n",
        "\n",
        "            # Forward propagation\n",
        "            activations = fwdPropagate_2(inputs_batch, weights, activ_func)\n",
        "\n",
        "            # Backward propagation\n",
        "            weights = backPropagate_2(inputs_batch, targets_batch, weights, activations, learning_rate, derivative)\n",
        "\n",
        "        # Compute outputs on training data\n",
        "        _, _, outputs_train, _, _ = fwdPropagate_2(inputs_train, weights, activ_func)\n",
        "        predicted_labels_train = np.argmax(outputs_train, axis=1)\n",
        "        training_accuracy = np.mean(predicted_labels_train == targets_train)\n",
        "\n",
        "        # Compute outputs on validation data\n",
        "        _, _, outputs_val, _, _ = fwdPropagate_2(inputs_val, weights, activ_func)\n",
        "        predicted_labels_val = np.argmax(outputs_val, axis=1)\n",
        "        validation_accuracy = np.mean(predicted_labels_val == targets_val)\n",
        "\n",
        "        # Check if validation accuracy improved\n",
        "        if validation_accuracy > best_validation_accuracy:\n",
        "            best_validation_accuracy = validation_accuracy\n",
        "            best_weights = weights.copy()\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        # Print statistics\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Training Accuracy: {training_accuracy}, Validation Accuracy: {validation_accuracy}\")\n",
        "\n",
        "        # Early stopping condition\n",
        "        if early_stopping_counter >= limit:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "    return best_weights\n",
        "\n",
        "# Train the network with early stopping criterion\n",
        "print(\"\\nTraining with early stopping criterion:\")\n",
        "trained_weights_early_stopping = train_with_early_stopping(mnist_traindata.reshape(-1, 784), mnist_trainlabel, relu, relu_derivative)\n",
        "test_accuracy_early_stopping = evaluate_2(mnist_testdata.reshape(-1, 784), mnist_testlabel, trained_weights_early_stopping, relu)\n",
        "print(f\"Test Accuracy with early stopping criterion: {test_accuracy_early_stopping:.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x5qIX_n4zPt"
      },
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1. The training stops early when the validation accuracy doesn't improve for a specified number of epochs (limit).\n",
        "\n",
        "\n",
        "2. Training and validation accuracies improve over epochs, and early stopping prevents overfitting. The test accuracy achieved with early stopping criterion is 0.9787.\n",
        "\n",
        "3. Among the three techniques, weight regularization achieves the highest test accuracy, followed by dropout and early stopping.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
